#!/usr/bin/env python3
"""
–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö SPNet –∏ STECCOM –≤ PostgreSQL –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
"""

import pandas as pd
import psycopg2
from psycopg2.extras import execute_batch
import glob
import os
from pathlib import Path
import logging
from datetime import datetime
import sys

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class PostgresDataLoader:
    def __init__(self, db_config):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≥—Ä—É–∑—á–∏–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        
        Args:
            db_config (dict): –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ PostgreSQL
        """
        self.db_config = db_config
        self.connection = None
        self.spnet_path = "/mnt/gdrive/ai_report/SPNet reports"
        self.steccom_path = "/mnt/gdrive/ai_report/STECCOMLLCRussiaSBD.AccessFees_reports"
    
    def connect(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL"""
        try:
            self.connection = psycopg2.connect(**self.db_config)
            self.connection.autocommit = False
            logger.info("–£—Å–ø–µ—à–Ω–æ–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL")
            return True
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ PostgreSQL: {e}")
            return False
    
    def is_file_loaded(self, file_name, table_name='spnet_traffic', file_path=None):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞, –∑–∞–≥—Ä—É–∂–µ–Ω –ª–∏ —Ñ–∞–π–ª —É–∂–µ –∏ –ø–æ–ª–Ω–æ—Å—Ç—å—é –ª–∏ –∑–∞–≥—Ä—É–∂–µ–Ω
        
        Args:
            file_name: –∏–º—è —Ñ–∞–π–ª–∞
            table_name: –∏–º—è —Ç–∞–±–ª–∏—Ü—ã
            file_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∑–∞–ø–∏—Å–µ–π)
        
        Returns:
            tuple: (is_loaded: bool, records_in_file: int, records_in_db: int)
        """
        if not self.connection:
            return (False, 0, 0)
        
        cursor = self.connection.cursor()
        records_in_file = 0
        records_in_db = 0
        
        try:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∑–∞–ø–∏—Å–∏ –≤ load_logs
            cursor.execute("""
                SELECT COUNT(*) FROM load_logs 
                WHERE LOWER(source_file) = LOWER(%s) 
                AND LOWER(table_name) = LOWER(%s)
                AND load_status = 'SUCCESS'
            """, (file_name, table_name))
            has_log_entry = cursor.fetchone()[0] > 0
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –≤ –±–∞–∑–µ
            cursor.execute(f"""
                SELECT COUNT(*) FROM {table_name}
                WHERE LOWER(source_file) = LOWER(%s)
            """, (file_name,))
            records_in_db = cursor.fetchone()[0]
            
            # –ï—Å–ª–∏ –µ—Å—Ç—å –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É, –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –≤ —Ñ–∞–π–ª–µ
            if file_path and Path(file_path).exists():
                try:
                    file_ext = Path(file_path).suffix.lower()
                    if file_ext == '.xlsx':
                        df = pd.read_excel(file_path, dtype=str, na_filter=False, engine='openpyxl')
                    else:
                        df = pd.read_csv(file_path, dtype=str, na_filter=False)
                    df = df.dropna(how='all')
                    records_in_file = len(df)
                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥—Å—á–∏—Ç–∞—Ç—å –∑–∞–ø–∏—Å–∏ –≤ —Ñ–∞–π–ª–µ {file_name}: {e}")
            
            # –§–∞–π–ª —Å—á–∏—Ç–∞–µ—Ç—Å—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º, –µ—Å–ª–∏:
            # 1. –ï—Å—Ç—å –∑–∞–ø–∏—Å—å –≤ load_logs
            # 2. –ï—Å—Ç—å –¥–∞–Ω–Ω—ã–µ –≤ —Ç–∞–±–ª–∏—Ü–µ
            # 3. –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π –≤ –±–∞–∑–µ >= –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∑–∞–ø–∏—Å–µ–π –≤ —Ñ–∞–π–ª–µ (–µ—Å–ª–∏ —É–¥–∞–ª–æ—Å—å –ø–æ–¥—Å—á–∏—Ç–∞—Ç—å)
            if has_log_entry and records_in_db > 0:
                if records_in_file > 0:
                    # –ï—Å–ª–∏ —É–¥–∞–ª–æ—Å—å –ø–æ–¥—Å—á–∏—Ç–∞—Ç—å –∑–∞–ø–∏—Å–∏ –≤ —Ñ–∞–π–ª–µ, —Å—Ä–∞–≤–Ω–∏–≤–∞–µ–º
                    is_loaded = records_in_db >= records_in_file
                else:
                    # –ï—Å–ª–∏ –Ω–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥—Å—á–∏—Ç–∞—Ç—å, —Å—á–∏—Ç–∞–µ–º –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–º
                    is_loaded = True
            else:
                is_loaded = False
            
            return (is_loaded, records_in_file, records_in_db)
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ load_logs: {e}")
            return (False, 0, 0)
        finally:
            cursor.close()
    
    def log_load_result(self, table_name, file_name, records_loaded, load_status='SUCCESS', error_message=None):
        """–õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ load_logs"""
        if not self.connection:
            return
        cursor = self.connection.cursor()
        try:
            cursor.execute("""
                INSERT INTO load_logs (
                    table_name, source_file, records_loaded, load_status, 
                    error_message, load_start_time, load_end_time, created_by
                ) VALUES (
                    %s, %s, %s, %s, %s, CURRENT_TIMESTAMP, CURRENT_TIMESTAMP, %s
                )
            """, (table_name, file_name, records_loaded, load_status, error_message, 'STREAMLIT_LOADER'))
            self.connection.commit()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –≤ load_logs: {e}")
            self.connection.rollback()
        finally:
            cursor.close()
    
    def load_spnet_files(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ SPNet CSV —Ñ–∞–π–ª–æ–≤ (–ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ)"""
        logger.info("="*80)
        logger.info("–ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –¥–∞–Ω–Ω—ã—Ö SPNet...")
        logger.info("="*80)
        logger.info(f"–ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ SPNet: {self.spnet_path}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        if not Path(self.spnet_path).exists():
            logger.error(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {self.spnet_path}")
            return False
        
        csv_files = glob.glob(f"{self.spnet_path}/*.csv") + glob.glob(f"{self.spnet_path}/*.xlsx")
        logger.info(f"–ù–∞–π–¥–µ–Ω–æ —Ñ–∞–π–ª–æ–≤: {len(csv_files)} (CSV + XLSX)")
        
        if not csv_files:
            logger.warning(f"–§–∞–π–ª—ã SPNet –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {self.spnet_path}")
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            try:
                dir_contents = list(Path(self.spnet_path).glob("*"))
                logger.info(f"–°–æ–¥–µ—Ä–∂–∏–º–æ–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {[f.name for f in dir_contents]}")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {e}")
            return False
        
        total_records = 0
        skipped_files = 0
        load_start_time = datetime.now()
        
        for file_path in csv_files:
            file_name = Path(file_path).name
            file_ext = Path(file_path).suffix.lower()
            logger.info(f"\n{'='*60}")
            logger.info(f"–§–∞–π–ª: {file_name} (—Ç–∏–ø: {file_ext})")
            logger.info(f"–ü–æ–ª–Ω—ã–π –ø—É—Ç—å: {file_path}")
            
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ —Ñ–∞–π–ª–∞
                if not Path(file_path).exists():
                    logger.error(f"–§–∞–π–ª –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {file_path}")
                    self.log_load_result('spnet_traffic', file_name, 0, 'FAILED', f"File not found: {file_path}")
                    continue
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–≥—Ä—É–∂–µ–Ω –ª–∏ —Ñ–∞–π–ª —É–∂–µ
                is_loaded, records_in_file, records_in_db = self.is_file_loaded(file_name, 'spnet_traffic', file_path)
                if is_loaded:
                    logger.info(f"‚è≠ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª (—É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω –ø–æ–ª–Ω–æ—Å—Ç—å—é): {file_name}")
                    if records_in_file > 0 and records_in_db > 0:
                        logger.info(f"   –ó–∞–ø–∏—Å–µ–π –≤ —Ñ–∞–π–ª–µ: {records_in_file:,}, –≤ –±–∞–∑–µ: {records_in_db:,}")
                    skipped_files += 1
                    continue
                elif records_in_db > 0:
                    logger.info(f"‚ö†Ô∏è –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é: {file_name}")
                    logger.info(f"   –ó–∞–ø–∏—Å–µ–π –≤ —Ñ–∞–π–ª–µ: {records_in_file:,}, –≤ –±–∞–∑–µ: {records_in_db:,} (–Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç {records_in_file - records_in_db:,})")
                    logger.info(f"   –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª...")
                
                logger.info(f"üîÑ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É —Ñ–∞–π–ª–∞: {file_name}")
                load_start = datetime.now()
                records_loaded = self.load_spnet_file(file_path)
                load_end = datetime.now()
                duration = (load_end - load_start).total_seconds()
                total_records += records_loaded
                
                # –õ–æ–≥–∏—Ä—É–µ–º —É—Å–ø–µ—à–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É
                self.log_load_result('spnet_traffic', file_name, records_loaded, 'SUCCESS')
                logger.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {records_loaded} –∑–∞–ø–∏—Å–µ–π –∑–∞ {duration:.2f} —Å–µ–∫")
                
            except Exception as e:
                error_msg = str(e)
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {file_path}: {error_msg}")
                import traceback
                logger.error(traceback.format_exc())
                # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É
                self.log_load_result('spnet_traffic', file_name, 0, 'FAILED', error_msg)
        
        load_end_time = datetime.now()
        duration = (load_end_time - load_start_time).total_seconds()
        
        logger.info(f"\n{'='*80}")
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ SPNet –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        logger.info(f"–í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {total_records:,} –∑–∞–ø–∏—Å–µ–π")
        logger.info(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ (—É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã): {skipped_files}")
        logger.info(f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {duration:.2f} —Å–µ–∫")
        logger.info(f"{'='*80}\n")
        
        return True
    
    def load_spnet_file(self, file_path):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–¥–Ω–æ–≥–æ SPNet CSV –∏–ª–∏ XLSX —Ñ–∞–π–ª–∞"""
        file_ext = Path(file_path).suffix.lower()
        
        # –ß–∏—Ç–∞–µ–º XLSX —Ñ–∞–π–ª—ã
        if file_ext == '.xlsx':
            try:
                # –ü—Ä–æ–±—É–µ–º –ø—Ä–æ—á–∏—Ç–∞—Ç—å XLSX —Ñ–∞–π–ª —Å —è–≤–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º –¥–≤–∏–∂–∫–∞
                # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã: —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º –≤ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–µ –∏–ª–∏ –±–µ–∑
                df = None
                for header_row in [0, None]:
                    try:
                        if header_row is not None:
                            df = pd.read_excel(file_path, dtype=str, na_filter=False, engine='openpyxl', header=header_row)
                        else:
                            # –ü—Ä–æ–±—É–µ–º –±–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞, –ø–æ—Ç–æ–º –Ω–∞–∑–Ω–∞—á–∏–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –≤—Ä—É—á–Ω—É—é
                            df = pd.read_excel(file_path, dtype=str, na_filter=False, engine='openpyxl', header=None)
                            # –ï—Å–ª–∏ –ø–µ—Ä–≤–∞—è —Å—Ç—Ä–æ–∫–∞ –ø–æ—Ö–æ–∂–∞ –Ω–∞ –∑–∞–≥–æ–ª–æ–≤–∫–∏, –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë
                            if not df.empty and len(df) > 0:
                                first_row = df.iloc[0].astype(str).tolist()
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –ø–æ—Ö–æ–∂–∏ –ª–∏ –∑–Ω–∞—á–µ–Ω–∏—è –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–∏ –Ω–∞ –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫
                                if any('contract' in str(v).lower() or 'imei' in str(v).lower() or 'total' in str(v).lower() for v in first_row):
                                    df.columns = first_row
                                    df = df.iloc[1:].reset_index(drop=True)
                        break
                    except Exception as e:
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å —Å header={header_row}: {e}")
                        continue
                
                if df is None:
                    raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å XLSX —Ñ–∞–π–ª –Ω–∏ —Å –æ–¥–Ω–∏–º –≤–∞—Ä–∏–∞–Ω—Ç–æ–º –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤")
                
                logger.info(f"–£—Å–ø–µ—à–Ω–æ –ø—Ä–æ—á–∏—Ç–∞–Ω XLSX —Ñ–∞–π–ª {file_path}: {len(df)} —Å—Ç—Ä–æ–∫, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫")
                logger.info(f"–ö–æ–ª–æ–Ω–∫–∏ –≤ —Ñ–∞–π–ª–µ: {list(df.columns)}")
                
                # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –Ω–∞–∑–≤–∞–Ω–∏—è –∫–æ–ª–æ–Ω–æ–∫ (—É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã, –ø—Ä–∏–≤–æ–¥–∏–º –∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–º—É –≤–∏–¥—É)
                df.columns = [str(col).strip() for col in df.columns]
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Ñ–∞–π–ª –Ω–µ –ø—É—Å—Ç–æ–π
                if df.empty:
                    logger.warning(f"XLSX —Ñ–∞–π–ª {file_path} –ø—É—Å—Ç")
                    return 0
                
                # –£–¥–∞–ª—è–µ–º –ø–æ–ª–Ω–æ—Å—Ç—å—é –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
                df = df.dropna(how='all')
                if df.empty:
                    logger.warning(f"XLSX —Ñ–∞–π–ª {file_path} —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏")
                    return 0
                
                logger.info(f"–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫: {len(df)} —Å—Ç—Ä–æ–∫")
                
            except ImportError as e:
                logger.error(f"–ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ openpyxl –¥–ª—è —á—Ç–µ–Ω–∏—è XLSX —Ñ–∞–π–ª–æ–≤: {e}")
                logger.error("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install openpyxl")
                raise
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è XLSX —Ñ–∞–π–ª–∞ {file_path}: {e}")
                import traceback
                logger.error(traceback.format_exc())
                raise
        else:
            # –ß–∏—Ç–∞–µ–º CSV —Ñ–∞–π–ª—ã
            try:
                df = pd.read_csv(file_path, dtype=str, na_filter=False)
                logger.info(f"–£—Å–ø–µ—à–Ω–æ –ø—Ä–æ—á–∏—Ç–∞–Ω CSV —Ñ–∞–π–ª {file_path}: {len(df)} —Å—Ç—Ä–æ–∫, {len(df.columns)} –∫–æ–ª–æ–Ω–æ–∫")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è CSV —Ñ–∞–π–ª–∞ {file_path}: {e}")
                raise
        
        # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–æ–ª–æ–Ω–∫–∏ –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –≤–∞—Ä–∏–∞–Ω—Ç–∞–º –Ω–∞–∑–≤–∞–Ω–∏—è
        def find_column(df, possible_names):
            """–ò—â–µ—Ç –∫–æ–ª–æ–Ω–∫—É –ø–æ —Ä–∞–∑–ª–∏—á–Ω—ã–º –≤–∞—Ä–∏–∞–Ω—Ç–∞–º –Ω–∞–∑–≤–∞–Ω–∏—è (—Å —É—á–µ—Ç–æ–º –ø—Ä–æ–±–µ–ª–æ–≤, —Ä–µ–≥–∏—Å—Ç—Ä–∞ –∏ —Ç.–¥.)"""
            df_cols_lower = {str(col).lower().strip(): col for col in df.columns}
            for name in possible_names:
                name_lower = name.lower().strip()
                # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                if name_lower in df_cols_lower:
                    return df_cols_lower[name_lower]
                # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (—É–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–µ–ª—ã, —Å–∫–æ–±–∫–∏ –∏ —Ç.–¥.)
                name_normalized = name_lower.replace(' ', '').replace('(', '').replace(')', '').replace('-', '').replace('_', '')
                for col_name, col_orig in df_cols_lower.items():
                    col_normalized = col_name.replace(' ', '').replace('(', '').replace(')', '').replace('-', '').replace('_', '')
                    if name_normalized in col_normalized or col_normalized in name_normalized:
                        return col_orig
            return None
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –∏ –ª–æ–≥–∏—Ä—É–µ–º
        required_columns = ['Contract ID', 'IMEI', 'Total Amount']
        missing_columns = []
        for col_name in required_columns:
            if find_column(df, [col_name]) is None:
                missing_columns.append(col_name)
        
        if missing_columns:
            logger.warning(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –≤ —Ñ–∞–π–ª–µ {file_path}: {missing_columns}")
            logger.info(f"–î–æ—Å—Ç—É–ø–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")
        
        df['source_file'] = Path(file_path).name
        df['load_date'] = datetime.now()
        df['created_by'] = 'SPNET_LOADER'
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–ø–∏—Å–∏ —Å –≥–∏–±–∫–∏–º –ø–æ–∏—Å–∫–æ–º –∫–æ–ª–æ–Ω–æ–∫
        records = []
        skipped_rows = 0
        for idx, row in df.iterrows():
            try:
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–∏–±–∫–∏–π –ø–æ–∏—Å–∫ –∫–æ–ª–æ–Ω–æ–∫
                total_rows_col = find_column(df, ['Total Rows', 'TotalRows', 'total_rows'])
                contract_id_col = find_column(df, ['Contract ID', 'ContractID', 'Contract_Id', 'contract_id'])
                imei_col = find_column(df, ['IMEI', 'imei'])
                sim_iccid_col = find_column(df, ['SIM (ICCID)', 'SIM(ICCID)', 'SIM_ICCID', 'sim_iccid', 'ICCID'])
                service_col = find_column(df, ['Service', 'service'])
                usage_type_col = find_column(df, ['Usage Type', 'UsageType', 'usage_type'])
                usage_col = find_column(df, ['Usage', 'usage'])
                usage_unit_col = find_column(df, ['Usage Unit', 'UsageUnit', 'usage_unit'])
                total_amount_col = find_column(df, ['Total Amount', 'TotalAmount', 'total_amount', 'Amount', 'amount'])
                bill_month_col = find_column(df, ['Bill Month', 'BillMonth', 'bill_month'])
                plan_name_col = find_column(df, ['Plan Name', 'PlanName', 'plan_name'])
                imsi_col = find_column(df, ['IMSI', 'imsi'])
                msisdn_col = find_column(df, ['MSISDN', 'msisdn'])
                actual_usage_col = find_column(df, ['Actual Usage', 'ActualUsage', 'actual_usage'])
                call_session_count_col = find_column(df, ['Call/Session Count', 'CallSessionCount', 'call_session_count'])
                sp_account_no_col = find_column(df, ['SP Account No', 'SPAccountNo', 'sp_account_no'])
                sp_name_col = find_column(df, ['SP Name', 'SPName', 'sp_name'])
                sp_reference_col = find_column(df, ['SP Reference', 'SPReference', 'sp_reference'])
                
                record = (
                    self.parse_number(row.get(total_rows_col) if total_rows_col else None),
                    row.get(contract_id_col) if contract_id_col else None,
                    row.get(imei_col) if imei_col else None,
                    row.get(sim_iccid_col) if sim_iccid_col else None,
                    row.get(service_col) if service_col else None,
                    row.get(usage_type_col) if usage_type_col else None,
                    self.parse_number(row.get(usage_col) if usage_col else None),
                    row.get(usage_unit_col) if usage_unit_col else None,
                    self.parse_number(row.get(total_amount_col) if total_amount_col else None),
                    self.parse_number(row.get(bill_month_col) if bill_month_col else None),
                    row.get(plan_name_col) if plan_name_col else None,
                    row.get(imsi_col) if imsi_col else None,
                    row.get(msisdn_col) if msisdn_col else None,
                    self.parse_number(row.get(actual_usage_col) if actual_usage_col else None),
                    self.parse_number(row.get(call_session_count_col) if call_session_count_col else None),
                    self.parse_number(row.get(sp_account_no_col) if sp_account_no_col else None),
                    row.get(sp_name_col) if sp_name_col else None,
                    row.get(sp_reference_col) if sp_reference_col else None,
                    row.get('source_file'),
                    row.get('load_date'),
                    row.get('created_by')
                )
                records.append(record)
            except Exception as e:
                skipped_rows += 1
                logger.warning(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å—Ç—Ä–æ–∫–∏ {idx} –≤ —Ñ–∞–π–ª–µ {file_path}: {e}")
                continue
        
        if skipped_rows > 0:
            logger.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ —Å—Ç—Ä–æ–∫ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ: {skipped_rows}")
        
        if not records:
            logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–≥–æ—Ç–æ–≤–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–π –∑–∞–ø–∏—Å–∏ –∏–∑ —Ñ–∞–π–ª–∞ {file_path}")
            return 0
        
        logger.info(f"–ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(records)} –∑–∞–ø–∏—Å–µ–π –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏")
        
        # –í—Å—Ç–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
        return self.insert_spnet_records(records)
    
    def insert_spnet_records(self, records):
        """–í—Å—Ç–∞–≤–∫–∞ –∑–∞–ø–∏—Å–µ–π SPNet –≤ PostgreSQL"""
        cursor = self.connection.cursor()
        
        try:
            insert_sql = """
            INSERT INTO spnet_traffic (
                total_rows, contract_id, imei, sim_iccid, service, usage_type,
                usage_bytes, usage_unit, total_amount, bill_month, plan_name,
                imsi, msisdn, actual_usage, call_session_count, sp_account_no,
                sp_name, sp_reference, source_file, load_date, created_by
            ) VALUES (
                %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,
                %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
            )
            """
            
            execute_batch(cursor, insert_sql, records)
            self.connection.commit()
            
            return len(records)
            
        except Exception as e:
            self.connection.rollback()
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—Å—Ç–∞–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö SPNet: {e}")
            raise
        finally:
            cursor.close()
    
    def load_steccom_files(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ STECCOM CSV —Ñ–∞–π–ª–æ–≤ (–ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç —É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã–µ)"""
        logger.info("="*80)
        logger.info("–ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –¥–∞–Ω–Ω—ã—Ö STECCOM...")
        logger.info("="*80)
        
        csv_files = glob.glob(f"{self.steccom_path}/*.csv")
        if not csv_files:
            logger.warning(f"CSV —Ñ–∞–π–ª—ã STECCOM –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ {self.steccom_path}")
            return False
        
        total_records = 0
        skipped_files = 0
        load_start_time = datetime.now()
        
        for file_path in csv_files:
            file_name = Path(file_path).name
            try:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–≥—Ä—É–∂–µ–Ω –ª–∏ —Ñ–∞–π–ª —É–∂–µ
                is_loaded, records_in_file, records_in_db = self.is_file_loaded(file_name, 'steccom_expenses', file_path)
                if is_loaded:
                    logger.info(f"\n‚è≠ –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–∞–π–ª (—É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω –ø–æ–ª–Ω–æ—Å—Ç—å—é): {file_name}")
                    if records_in_file > 0 and records_in_db > 0:
                        logger.info(f"   –ó–∞–ø–∏—Å–µ–π –≤ —Ñ–∞–π–ª–µ: {records_in_file:,}, –≤ –±–∞–∑–µ: {records_in_db:,}")
                    skipped_files += 1
                    continue
                elif records_in_db > 0:
                    logger.info(f"\n‚ö†Ô∏è –§–∞–π–ª –∑–∞–≥—Ä—É–∂–µ–Ω –Ω–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é: {file_name}")
                    logger.info(f"   –ó–∞–ø–∏—Å–µ–π –≤ —Ñ–∞–π–ª–µ: {records_in_file:,}, –≤ –±–∞–∑–µ: {records_in_db:,} (–Ω–µ —Ö–≤–∞—Ç–∞–µ—Ç {records_in_file - records_in_db:,})")
                    logger.info(f"   –ü–µ—Ä–µ–∑–∞–≥—Ä—É–∂–∞–µ–º —Ñ–∞–π–ª...")
                
                logger.info(f"\n–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ñ–∞–π–ª: {file_name}")
                load_start = datetime.now()
                records_loaded = self.load_steccom_file(file_path)
                load_end = datetime.now()
                total_records += records_loaded
                
                # –õ–æ–≥–∏—Ä—É–µ–º —É—Å–ø–µ—à–Ω—É—é –∑–∞–≥—Ä—É–∑–∫—É
                self.log_load_result('steccom_expenses', file_name, records_loaded, 'SUCCESS')
                logger.info(f"‚úì –ó–∞–≥—Ä—É–∂–µ–Ω–æ {records_loaded} –∑–∞–ø–∏—Å–µ–π")
                
            except Exception as e:
                error_msg = str(e)
                logger.error(f"‚úó –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–∞–π–ª–∞ {file_path}: {error_msg}")
                # –õ–æ–≥–∏—Ä—É–µ–º –æ—à–∏–±–∫—É
                self.log_load_result('steccom_expenses', file_name, 0, 'FAILED', error_msg)
        
        load_end_time = datetime.now()
        duration = (load_end_time - load_start_time).total_seconds()
        
        logger.info(f"\n{'='*80}")
        logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ STECCOM –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
        logger.info(f"–í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ: {total_records:,} –∑–∞–ø–∏—Å–µ–π")
        logger.info(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ —Ñ–∞–π–ª–æ–≤ (—É–∂–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã): {skipped_files}")
        logger.info(f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {duration:.2f} —Å–µ–∫")
        logger.info(f"{'='*80}\n")
        
        return True
    
    def load_steccom_file(self, file_path):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –æ–¥–Ω–æ–≥–æ STECCOM CSV —Ñ–∞–π–ª–∞"""
        # –ß–∏—Ç–∞–µ–º CSV
        df = pd.read_csv(file_path, dtype=str, na_filter=False)
        
        df['source_file'] = Path(file_path).name
        df['load_date'] = datetime.now()
        df['created_by'] = 'STECCOM_LOADER'
        
        # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∑–∞–ø–∏—Å–∏
        records = []
        for _, row in df.iterrows():
            record = (
                self.parse_date(row.get('Invoice Date')),
                row.get('Company Name') or None,
                self.parse_number(row.get('Company Number')),
                self.parse_number(row.get('Settling Period')),
                row.get('Fee Type') or None,
                row.get('Contract ID') or None,
                row.get('IMSI/ISDNA') or None,
                row.get('ICC-ID/IMEI') or None,
                self.parse_date(row.get('Activation Date')),
                self.parse_date(row.get('Transaction Date')),
                row.get('Service') or None,
                row.get('Rate Type') or None,
                row.get('Plan/Discount') or None,
                row.get('Description') or None,
                self.parse_number(row.get('Prorated Days')),
                self.parse_number(row.get('Amount')),
                row.get('Group') or None,
                row.get('source_file'),
                row.get('load_date'),
                row.get('created_by')
            )
            records.append(record)
        
        # –í—Å—Ç–∞–≤–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
        return self.insert_steccom_records(records)
    
    def insert_steccom_records(self, records):
        """–í—Å—Ç–∞–≤–∫–∞ –∑–∞–ø–∏—Å–µ–π STECCOM –≤ PostgreSQL"""
        cursor = self.connection.cursor()
        
        try:
            insert_sql = """
            INSERT INTO steccom_expenses (
                invoice_date, company_name, company_number, settling_period,
                fee_type, contract_id, imsi_isdna, icc_id_imei, activation_date,
                transaction_date, service, rate_type, plan_discount, description,
                prorated_days, amount, group_name, source_file, load_date, created_by
            ) VALUES (
                %s, %s, %s, %s, %s, %s, %s, %s, %s, %s,
                %s, %s, %s, %s, %s, %s, %s, %s, %s, %s
            )
            """
            
            execute_batch(cursor, insert_sql, records)
            self.connection.commit()
            
            return len(records)
            
        except Exception as e:
            self.connection.rollback()
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—Å—Ç–∞–≤–∫–µ –¥–∞–Ω–Ω—ã—Ö STECCOM: {e}")
            raise
        finally:
            cursor.close()
    
    def parse_date(self, date_str):
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–∞—Ç—ã"""
        if not date_str or str(date_str).strip() == '':
            return None
        
        date_formats = [
            '%Y/%m/%d',
            '%d.%m.%Y',
            '%Y-%m-%d',
            '%d/%m/%Y'
        ]
        
        for fmt in date_formats:
            try:
                return datetime.strptime(str(date_str).strip(), fmt).date()
            except:
                continue
        
        return None
    
    def parse_number(self, value):
        """–ü–∞—Ä—Å–∏–Ω–≥ —á–∏—Å–ª–æ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è"""
        if not value or str(value).strip() == '':
            return None
        
        try:
            clean_value = str(value).strip()
            
            # –ù–∞—É—á–Ω–∞—è –Ω–æ—Ç–∞—Ü–∏—è
            if 'E+' in clean_value or 'E-' in clean_value:
                clean_value = clean_value.replace(',', '.')
            
            # –£–±–∏—Ä–∞–µ–º –≤–∞–ª—é—Ç–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
            clean_value = clean_value.replace('$', '').replace('‚Ç¨', '').replace(' ', '')
            
            # –£–±–∏—Ä–∞–µ–º –Ω–µ—á–∏—Å–ª–æ–≤—ã–µ —Å–∏–º–≤–æ–ª—ã
            import re
            clean_value = re.sub(r'[^\d\.\-\+E]', '', clean_value)
            
            if clean_value and clean_value != '-':
                return float(clean_value)
            
            return None
        except:
            return None
    
    def get_statistics(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∑–∞–≥—Ä—É–∑–∫–∏"""
        cursor = self.connection.cursor()
        
        try:
            logger.info("\n" + "="*80)
            logger.info("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –ó–ê–ì–†–£–ñ–ï–ù–ù–´–• –î–ê–ù–ù–´–•")
            logger.info("="*80)
            
            # SPNet —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            cursor.execute("SELECT COUNT(*) FROM spnet_traffic")
            spnet_count = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(DISTINCT imei) FROM spnet_traffic WHERE imei IS NOT NULL")
            spnet_imeis = cursor.fetchone()[0]
            
            cursor.execute("SELECT SUM(usage_bytes) FROM spnet_traffic")
            spnet_usage = cursor.fetchone()[0] or 0
            
            cursor.execute("SELECT SUM(total_amount) FROM spnet_traffic")
            spnet_amount = cursor.fetchone()[0] or 0
            
            logger.info(f"\nSPNET_TRAFFIC:")
            logger.info(f"  –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {spnet_count:,}")
            logger.info(f"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö IMEI: {spnet_imeis:,}")
            logger.info(f"  –û–±—â–∏–π —Ç—Ä–∞—Ñ–∏–∫: {spnet_usage:,.0f} –±–∞–π—Ç ({spnet_usage/1000/1000:.2f} MB)")
            logger.info(f"  –û–±—â–∞—è —Å—É–º–º–∞: ${spnet_amount:,.2f}")
            
            # STECCOM —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            cursor.execute("SELECT COUNT(*) FROM steccom_expenses")
            steccom_count = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(DISTINCT icc_id_imei) FROM steccom_expenses WHERE icc_id_imei IS NOT NULL")
            steccom_imeis = cursor.fetchone()[0]
            
            cursor.execute("SELECT SUM(amount) FROM steccom_expenses")
            steccom_amount = cursor.fetchone()[0] or 0
            
            logger.info(f"\nSTECCOM_EXPENSES:")
            logger.info(f"  –í—Å–µ–≥–æ –∑–∞–ø–∏—Å–µ–π: {steccom_count:,}")
            logger.info(f"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö IMEI: {steccom_imeis:,}")
            logger.info(f"  –û–±—â–∞—è —Å—É–º–º–∞: ${steccom_amount:,.2f}")
            
            # –¢–∞—Ä–∏—Ñ–Ω—ã–µ –ø–ª–∞–Ω—ã
            cursor.execute("SELECT plan_code, plan_name, COUNT(*) FROM spnet_traffic WHERE plan_name IS NOT NULL GROUP BY plan_code, plan_name")
            plans = cursor.fetchall()
            
            logger.info(f"\n–¢–ê–†–ò–§–ù–´–ï –ü–õ–ê–ù–´:")
            for plan in plans:
                logger.info(f"  {plan[0] or 'Unknown'}: {plan[1]} - {plan[2]:,} –∑–∞–ø–∏—Å–µ–π")
            
            logger.info("\n" + "="*80 + "\n")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏: {e}")
        finally:
            cursor.close()
    
    def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è"""
        if self.connection:
            self.connection.close()
            logger.info("–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ PostgreSQL –∑–∞–∫—Ä—ã—Ç–æ\n")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è PostgreSQL (–∞–Ω–∞–ª–æ–≥ Oracle billing7/billing@bm7)
    db_config = {
        'dbname': 'billing',
        'user': 'postgres',
        'password': '1234',
        'host': 'localhost',
        'port': 5432
    }
    
    logger.info("="*80)
    logger.info("–ó–ê–ì–†–£–ó–ö–ê –î–ê–ù–ù–´–• –í POSTGRESQL")
    logger.info("="*80)
    
    loader = PostgresDataLoader(db_config)
    
    try:
        # –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ
        if not loader.connect():
            logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ PostgreSQL")
            return False
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ SPNet
        loader.load_spnet_files()
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ STECCOM
        loader.load_steccom_files()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        loader.get_statistics()
        
        logger.info("‚úì –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
        return True
        
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        import traceback
        traceback.print_exc()
        return False
    finally:
        loader.close()


if __name__ == '__main__':
    success = main()
    sys.exit(0 if success else 1)

